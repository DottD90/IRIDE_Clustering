#!/usr/bin/env python3
"""
compute_aac.py
Written by: Enrico Ciraci' - 02/2024

Compute Active Areas Deformation Clustering (AAC) for the selected
area of interest. Deformation data are extracted from the Sentinel-1
bursts indexed by the input file generated by the `index_bursts.py` script.

Script usage:
usage: Active Areas Deformation Clustering
    [-h] [-K KMEANS_CLASSES [KMEANS_CLASSES ...]] [-T VEL_THRESHOLD
    [VEL_THRESHOLD ...]] [-E EPS [EPS ...]] [-M MIN_SAMPLES [MIN_SAMPLES ...]]
    [-p PID_BASE [PID_BASE ...]] [-b BUFFER [BUFFER ...]]
    [-i IN_FIELD [IN_FIELD ...]] [-o OUT_FIELD [OUT_FIELD ...]] [-j JOBS]
    in_path [in_path ...]

Default parameters are adapted to CSK SVC01 products.

positional arguments:
  in_path               List of space-separated shapefiles, CSV or
    ZIP containing the CSV to be processed at once

options:
  -h, --help            show this help message and exit
  -K KMEANS_CLASSES [KMEANS_CLASSES ...], --kmeans-classes KMEANS_CLASSES
            [KMEANS_CLASSES ...]
             Number of classes for k-Means velocity clustering
             (broadcast to the size of input paths)

  -T VEL_THRESHOLD [VEL_THRESHOLD ...], --vel-threshold
            VEL_THRESHOLD [VEL_THRESHOLD ...]
            Velocity threshold applied to clusters
            (broadcast to the size of input paths)

  -E EPS [EPS ...], --eps EPS [EPS ...]
            epsilon parameter for DBSCAN algorithm and alphashape
            (broadcast to the size of input paths)

  -M MIN_SAMPLES [MIN_SAMPLES ...], --min-samples MIN_SAMPLES [MIN_SAMPLES ...]
            minimum number of samples, fed to the DBSCAN algorithm
            (broadcast to the size of input paths)
  -P PID_BASE [PID_BASE ...], --pid-base PID_BASE [PID_BASE ...]
            base string for PID construction
            (broadcast to the size of input paths)

  -B BUFFER [BUFFER ...], --buffer BUFFER [BUFFER ...]
        buffer used to dilate polygons
        (broadcast to the size of input paths)

  -I IN_FIELD [IN_FIELD ...], --in-field IN_FIELD [IN_FIELD ...]
        input field used for clustering (broadcast to the size of input paths)

  -O OUT_FIELD [OUT_FIELD ...], --out-field OUT_FIELD [OUT_FIELD ...]
        name of the field saved on the output shapefile
        (broadcast to the size of input paths)

    -C, --compress        Compress output shapefile - .zip format.

Python Dependencies:
    - pandas:Python Data Analysis Library
        https://pandas.pydata.org/
    - geopandas: Python tools for working with geospatial data in python
        https://geopandas.org/
    - numpy: The fundamental package for scientific computing with Python
        https://numpy.org/
    - scikit-learn: Machine Learning in Python
        https://scikit-learn.org/stable/
     - alphashape: Alpha Shape Toolbox
        https://pypi.org/project/alphashape/
"""
# - Python modules
import os
import argparse
import warnings
from datetime import datetime
from multiprocessing import Pool
from functools import partial
# - External modules
import geopandas as gpd
from active_areas_clustering import process_burst


def main() -> None:
    # - Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Compute Active Areas Clustering (AAC) for the selected "
                    "area of interest."
    )
    # - burst_file: Sentinel-1 burst index file file
    parser.add_argument('index_file', type=str,
                        help='Index file containing the list of'
                             'bursts available over the AOI.')
    # - Optional arguments
    parser.add_argument("-K", "--kmeans-classes",
                        type=int, default=21, nargs="+",
                        help="Number of classes for k-Means "
                             "velocity clustering (broadcast to the "
                             "size of input paths)")

    parser.add_argument("-T", "--vel-threshold", type=float,
                        default=1.25, nargs="+",
                        help="Velocity threshold applied to clusters "
                             "(broadcast to the size of input paths)")

    parser.add_argument("-E", "--eps", type=float,
                        default=25, nargs="+",
                        help="epsilon parameter for DBSCAN algorithm and "
                             "alphashape (broadcast to the size of "
                             "input paths)")

    parser.add_argument("-M", "--min-samples", type=int,
                        default=10, nargs="+",
                        help="minimum number of samples, "
                             "fed to the DBSCAN algorithm "
                             "(broadcast to the size of input paths)")

    parser.add_argument("-P", "--pid-base", type=str,
                        default="bDG1", nargs="+",
                        help="base string for PID construction "
                             "(broadcast to the size of input paths)")

    parser.add_argument("-B", "--buffer", type=float,
                        default=12.5, nargs="+",
                        help="buffer used to dilate polygons "
                             "(broadcast to the size of input paths)")

    parser.add_argument("-I", "--in-field", type=str,
                        default="mean_vel", nargs="+",
                        choices=["mean_vel", "acc"],
                        help="input field used for clustering "
                             "(broadcast to the size of input paths)")

    parser.add_argument("-O", "--out-field", type=str,
                        default="mean_vel", nargs="+",
                        help="name of the field saved on the output shapefile "
                             "(broadcast to the size of input paths)")

    parser.add_argument("-C", "--compress", action="store_true",
                        help="Compress output shapefile - .zip format")
    args = parser.parse_args()

    # - Verify if reference file exists
    if not os.path.exists(args.index_file):
        raise FileNotFoundError(f"Index file {args.index_file} "
                                f"does not exist.")
    # - Read the index file with geopandas
    index_gdf = gpd.read_file(args.index_file).to_crs("EPSG:4326")

    # - Extract the list of tracks
    tracks = index_gdf["Track"].unique()
    print(f"# - Number of available tracks: {len(tracks)}")

    # - Process each track
    for track in tracks:
        print(f"# - Processing track: {track}")
        # - Filter the index file by track
        track_gdf = index_gdf[index_gdf["Track"] == track]
        track_shape = track_gdf.shape[0]

        # - Ignore busts with Path set to None
        track_gdf = track_gdf[track_gdf["Path"] != "None"]

        if track_gdf.shape[0] == 0:
            print(f"# - No input file found for track: {track}")
            continue

        if track_gdf.shape[0] < track_shape:
            # - Rise Warning
            warnings.warn(f"# - Missing Input file for track: {track}")

        # - Process the track
        in_path = list(track_gdf['Path'])
        n_proc = len(in_path)

        with Pool(n_proc) as p:
            kwargs = {'kmeans_classes': args.kmeans_classes,
                      'in_field_threshold': args.vel_threshold,
                      'eps': args.eps,
                      'min_samples': args.min_samples,
                      'pid_base': args.pid_base,
                      'buffer': args.buffer,
                      'in_field': args.in_field,
                      'out_field': args.out_field,
                      'compress': args.compress}
            map_func = partial(process_burst, **kwargs)
            p_ts_list = p.map(map_func, in_path)


if __name__ == "__main__":
    start_time = datetime.now()
    main()
    end_time = datetime.now()
    print(f"# - Computation Time: {end_time - start_time}")
